{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "floating-madness",
   "metadata": {},
   "source": [
    "# Evaluating Simpler Models on collected data\n",
    "\n",
    "Because a Neural Net seems like overkill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "from collections import defaultdict\n",
    "from tempfile import NamedTemporaryFile\n",
    "from typing import List, Tuple\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.ticker import AutoMinorLocator\n",
    "\n",
    "from optpresso.data.df import (\n",
    "    dataframe_from_csv,\n",
    "    convert_datetime_to_epoch_time,\n",
    "    find_columns_to_drop,\n",
    "    prepare_df_for_modeling,\n",
    ")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# On linux HistGradientBoostingRegressor can freak out with too many threads\n",
    "# https://github.com/scikit-learn/scikit-learn/issues/16016\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea658c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_epoch_to_days(epoch_date):\n",
    "    return epoch_date // (3600 * 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU, fast enough without\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "DATA_DOWNLOAD_URL = \"https://docs.google.com/spreadsheets/d/1dDfuIq74pjELNtfb_0sdiNnG3qFKrfqC2sax7ewrWYg/export?format=csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8800e79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(df):\n",
    "    df = df.copy()\n",
    "    for char in [\" \", \"(\", \")\", \"%\", \"/\", \"\\\\\", '\"']:\n",
    "        df.columns = df.columns.str.strip().str.lower().str.replace(char, \"\", regex=False)\n",
    "    df.dropna(subset=[\"time\"], inplace=True)\n",
    "    df = df[pd.to_numeric(df[\"grindersetting\"], errors=\"coerce\").notnull()]\n",
    "    roast_date = pd.to_datetime(\n",
    "        df.roastdate, infer_datetime_format=True, utc=True\n",
    "    ).map(convert_datetime_to_epoch_time)\n",
    "    df[\"date\"] = pd.to_datetime(df.date, infer_datetime_format=True, utc=True).map(\n",
    "        convert_datetime_to_epoch_time\n",
    "    )\n",
    "    df[\"timesinceroast\"] = convert_epoch_to_days(df[\"date\"] - roast_date)\n",
    "    df[\"grindersetting\"] = df[\"grindersetting\"].astype(np.float64)\n",
    "    df[[\"roaster\", \"coffee\"]] = df[\"coffee\"].str.lower().str.split(\"-\", n=1, expand=True)\n",
    "    df[\"coffee\"] = df[\"coffee\"].astype(str).replace(\"None\", \"\").str.strip()\n",
    "    df[\"coffee_words\"] = df[\"coffee\"].str.split(\" \").map(lambda x: [y for y in x if not y.isdigit() and len(y) > 3])\n",
    "    df[\"machineprofile\"] = (\n",
    "        df[\"machineprofile\"].fillna(\"\").replace(\"\", \"default\").str.lower()\n",
    "    )\n",
    "    df[\"basket\"] = (\n",
    "        df[\"basket\"].fillna(\"default\").str.lower().str.strip()\n",
    "    )\n",
    "    df[\"roaster\"] = df[\"roaster\"].astype(str).replace(\"None\", \"\").str.lower().str.strip()\n",
    "    df[\"brewratio\"] = df[\"output\"] / df[\"coffeegrams\"]\n",
    "    df[\"roastdate\"] = df[\"roastdate\"].fillna(\"\").str.strip()\n",
    "    df[\"filter\"] = df[\"filter\"].fillna(\"\").str.lower().str.strip()\n",
    "    return df\n",
    "\n",
    "def build_encoders(frame, fields):\n",
    "    encs = {}\n",
    "    for col in fields:\n",
    "        if col not in df:\n",
    "            print(\"No such column to encode\", col)\n",
    "            continue\n",
    "        if not isinstance(frame[col], object) and len(frame[col].unique()) <= 1:\n",
    "            print(f\"Column {col} has 1 or fewer values, not encoding\")\n",
    "            continue\n",
    "        if isinstance(frame[col].values[0], str):\n",
    "            encoder = OrdinalEncoder(\n",
    "                handle_unknown=\"use_encoded_value\", unknown_value=len(frame[col].unique())\n",
    "            )\n",
    "            encoder.fit(frame[col].values.reshape(-1, 1))\n",
    "        elif isinstance(frame[col].values[0], list):\n",
    "            encoder = MultiLabelBinarizer()\n",
    "            encoder.fit(frame[col].values)\n",
    "        else:\n",
    "            raise TypeError(\"Unknown type for encoding\", col, type(frame[col].values[0]))\n",
    "        encs[col] = encoder\n",
    "    return encs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe_from_csv(DATA_DOWNLOAD_URL)\n",
    "df = prepare_dataframe(df)\n",
    "rng = np.random.default_rng(814)\n",
    "\n",
    "encoded_cols = (\"grinder\", \"coffee\", \"roaster\", \"machine\", \"machineprofile\", \"coffee_words\", \"roastdate\", \"basket\", \"filter\")\n",
    "\n",
    "\n",
    "# Drop fields that aren't going to be encoded or are a pandas type\n",
    "for field in df.columns:\n",
    "    if field in encoded_cols:\n",
    "        continue\n",
    "    if df[field].dtype == object:\n",
    "        print(\"Dropping\", field)\n",
    "        df = df.drop([field], axis=1)\n",
    "        \n",
    "for col in find_columns_to_drop(df):\n",
    "    if col in encoded_cols:\n",
    "        continue\n",
    "    print(\"Dropping\", col)\n",
    "    df = df.drop(col, axis=1)\n",
    "\n",
    "for col in [\"arbitraryrating1-10\"]:\n",
    "    if col in df.columns:\n",
    "        print(\"Dropping\", col)\n",
    "        df = df.drop(col, axis=1)\n",
    "# Shuffle, in case ordering matters\n",
    "df = df.sample(frac=1, random_state=rng).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f34aa1a",
   "metadata": {},
   "source": [
    "## Use distribution of grams of input coffee to enhance data\n",
    "\n",
    "Initially values were just the grams weighed before grinding. After getting a more precise scale, data has been collected after grinding/prep, want to see if using that data can enhance the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b06ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_field(\n",
    "    frame, field: str, groupby: List[str], mean=0.0, std=1.0, samples: int = 3, seed: int = 814\n",
    "):\n",
    "    to_sample = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Duplicate a bunch of samples\n",
    "    for name, subdf in frame.groupby(groupby):\n",
    "        to_return = pd.concat([subdf] * samples)\n",
    "        noise = rng.normal(mean, std, size=len(to_return))\n",
    "        to_return[field] = (to_return[field].values + noise).tolist()\n",
    "        to_sample.append(to_return)\n",
    "    new_data = pd.concat(to_sample).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    if field in [\"coffeegrams\", \"output\"]:\n",
    "        new_data.brewratio = new_data.output / new_data.coffeegrams\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5760854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_regressor(estimators: int = 125):\n",
    "    return HistGradientBoostingRegressor(\n",
    "#         loss='poisson',\n",
    "        max_iter=estimators, random_state=814\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-gender",
   "metadata": {},
   "source": [
    "## Evaluate using GradientBoosting to make predictions\n",
    "\n",
    "It is feasible to build a personalized GradientBoost model, and this is to do a basic proof of concept to see if it isn't totally worthless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c59aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_difference(df, field):\n",
    "    diff = np.diff(np.sort(np.array(df[field].unique())))\n",
    "    min_difference = np.round(np.min(diff[diff > 0]), 3)\n",
    "    return min_difference\n",
    "\n",
    "def plot_grinder_predictions(grinder: str, prediction_field: str, y, y_pred):\n",
    "    fig, (ax, ax2) = plt.subplots(ncols=2, dpi=150, figsize=(9, 4.5))\n",
    "    fig.suptitle(f\"{grinder}\\n{prediction_field.title()}\")\n",
    "    grinder_df = df[df.grinder == grinder]\n",
    "    min_grinder_difference = get_min_difference(grinder_df, prediction_field)\n",
    "    ax.scatter(\n",
    "        y,\n",
    "        y_pred,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    lowest_point = min(np.min(y), np.min(y_pred)) - min_grinder_difference\n",
    "    max_point = max(np.max(y), np.max(y_pred)) + min_grinder_difference\n",
    "    ax.set_title(\"Predictions\")\n",
    "\n",
    "    ax.set_xlabel(\"Experimental\")\n",
    "    ax.set_ylabel(\"Prediction\")\n",
    "    min_fit_point = np.array([lowest_point] * 2)\n",
    "    max_fit_point = np.array([max_point] * 2)\n",
    "    ax.set_xlim(min_fit_point[0], max_fit_point[1])\n",
    "    ax.axline(min_fit_point, slope=1, color=\"g\", linestyle=\"--\", label=\"fit\")\n",
    "    \n",
    "    for factor in [1.0, 2.0]:\n",
    "        ax.fill_between(\n",
    "            [min_fit_point[0], max_fit_point[0]],\n",
    "            [min_fit_point[1] - min_grinder_difference * factor, max_fit_point[1] - min_grinder_difference * factor],\n",
    "            [min_fit_point[1] + min_grinder_difference * factor, max_fit_point[1] + min_grinder_difference * factor],\n",
    "            color=\"gray\",\n",
    "            label=\"fit\",\n",
    "            alpha=0.25\n",
    "        )\n",
    "    ax.annotate(\n",
    "        \"RMSE = {:.2f}\".format(np.sqrt(mean_squared_error(y, y_pred))),\n",
    "        (1.03, 0.01),\n",
    "        xycoords=\"axes fraction\",\n",
    "    )\n",
    "    ax.annotate(\n",
    "        \"MUE = {:.2f}\".format(mean_absolute_error(y, y_pred)),\n",
    "        (1.03, 0.07),\n",
    "        xycoords=\"axes fraction\",\n",
    "    )\n",
    "    ax.annotate(\n",
    "        \"MAD = {:.2f}\".format(median_abs_deviation(y - y_pred)),\n",
    "        (1.03, 0.12),\n",
    "        xycoords=\"axes fraction\",\n",
    "    )\n",
    "    \n",
    "\n",
    "    ax2.hist(y_pred - y, bins=30, density=True)\n",
    "    ax2.set_title(f\"Residuals\")\n",
    "    ax2.set_xlabel(\"Error\")\n",
    "    ax2.set_ylabel(\"Density\")\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples values for the training set\n",
    "enhance_sample = True\n",
    "\n",
    "estimators = 1000\n",
    "num_days = 4\n",
    "days_in_seconds = 24 * 60 * 60 * num_days\n",
    "data = df.copy()\n",
    "latest_date = data.date.max()\n",
    "# Training data is everything num_days before the most recent pull\n",
    "train = data[data.date < latest_date - days_in_seconds]\n",
    "test = data.drop(train.index)\n",
    "\n",
    "if enhance_sample:\n",
    "    sampled_coffee = resample_field(train, \"coffeegrams\", [\"grinder\", \"coffee\"], std=0.1, samples=2)\n",
    "#     sampled_grind = resample_field(train, \"grindersetting\", [\"grinder\", \"coffee\", \"machineprofile\"], std=0.05, samples=2)\n",
    "    train = pd.concat([train, sampled_coffee])\n",
    "for prediction_field in [\"grindersetting\", \"time\"]:\n",
    "    print(\"Prediction\", prediction_field)\n",
    "    for dropped in [(\"date\", \"brewratio\", \"roastdate\")]:\n",
    "        print(\"With Dropped columns:\", dropped)\n",
    "        training_encs = build_encoders(train, encoded_cols)\n",
    "        x, y = prepare_df_for_modeling(\n",
    "            train, prediction_field, drop_cols=dropped, encoders=training_encs\n",
    "        )\n",
    "        regr = construct_regressor(estimators=estimators)\n",
    "        regr.fit(x, y)\n",
    "        for grinder in test.grinder.unique():\n",
    "            grinder_test = test[test.grinder == grinder]\n",
    "            x_test, y_test = prepare_df_for_modeling(\n",
    "                grinder_test, prediction_field, drop_cols=dropped, encoders=training_encs\n",
    "            )\n",
    "            yfit = regr.predict(x_test)\n",
    "            plot_grinder_predictions(grinder, prediction_field, y_test, yfit)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-excellence",
   "metadata": {},
   "source": [
    "## Trying it out on new data\n",
    "\n",
    "What happens if we throw 'novel' coffee at it, how well will it predict it across grinders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e30e276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_value_exists(field, ref_df, df):\n",
    "    ref_val = ref_df[field].values[0]\n",
    "    if ref_val not in df[field].values:\n",
    "        print(f\"No matching {field} '{ref_val}' found in dataframe\")\n",
    "        print(\"-----\")\n",
    "\n",
    "def percent_diff(x, y):\n",
    "    return np.abs(x - y) / x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31940021",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Select the number of estimators and whether to try and generate more samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ee040",
   "metadata": {},
   "outputs": [],
   "source": [
    "enhance_sample = True\n",
    "estimators = 1000\n",
    "dropped = (\"date\", \"brewratio\", \"roastdate\")\n",
    "\n",
    "training = df\n",
    "if enhance_sample:\n",
    "    sampled_coffee = resample_field(training, \"coffeegrams\", [\"grinder\", \"coffee\"], std=0.1, samples=2)\n",
    "#     sampled_output = resample_field(\n",
    "#         training, \"output\", [\"machine\", \"machineprofile\", \"grinder\", \"coffee\"], samples=1\n",
    "#     )\n",
    "    sampled_grind = resample_field(training, \"grindersetting\", [\"grinder\", \"coffee\", \"machineprofile\"], std=0.05, samples=2)\n",
    "    training = pd.concat([training, sampled_coffee, sampled_grind])\n",
    "\n",
    "inference_encoders = build_encoders(training, encoded_cols)\n",
    "models_by_label = {}\n",
    "for label in [\"grindersetting\", \"time\"]:\n",
    "    x, y = prepare_df_for_modeling(\n",
    "        training, label, drop_cols=dropped, encoders=inference_encoders\n",
    "    )\n",
    "    regr = construct_regressor(estimators)\n",
    "    regr.fit(x, y)\n",
    "    models_by_label[label] = regr\n",
    "    print(f\"Trained model {label} with {len(y)} data points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8b607c",
   "metadata": {},
   "source": [
    "## Run Model Prospectively\n",
    "\n",
    "Modify the data dictionary to run the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    \"coffee\": \"Woodcat - Colombia Huila Caturra Chiroso Washed\",\n",
    "    \"coffeegrams\": 19.0,\n",
    "    \"roastdate\": \"07/11/23\",\n",
    "    \"time\": 50,\n",
    "    \"output\": 36.0,\n",
    "    \"machine\": \"Decent DE1PRO\",\n",
    "    \"machineprofile\": \"Londonium\",\n",
    "    \"grinder\": None,  # Leave None to predict for all grinders,\n",
    "    \"basket\": \"Weber Unibasket\",\n",
    "    \"filter\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "data[\"date\"] = date.today().strftime(\"%m/%d/%Y\")\n",
    "data[\"grindersetting\"] = 0.5  # Meaningless value\n",
    "\n",
    "data_df = pd.DataFrame.from_dict([data])\n",
    "data_df = prepare_dataframe(data_df)\n",
    "\n",
    "for field in [\"roaster\", \"coffee\", \"machine\", \"machineprofile\", \"basket\"]:\n",
    "    check_value_exists(field, data_df, df)\n",
    "\n",
    "profile_df = training[\n",
    "    (training.machine == data_df.machine.values[0])\n",
    "    & (training.machineprofile == data_df.machineprofile.values[0])\n",
    "]\n",
    "\n",
    "weight_values = np.array(df.coffeegrams.unique())\n",
    "# If greater than 2% difference in weight, ignore\n",
    "valid_weights_idxs = np.argwhere(percent_diff(data[\"coffeegrams\"], weight_values) < 0.02).reshape(-1)\n",
    "weight_values = weight_values[valid_weights_idxs]\n",
    "\n",
    "max_profile_time = max(min(60, profile_df.time.max()), data[\"time\"])\n",
    "step_size = 5\n",
    "min_profile_time = max(\n",
    "    step_size, profile_df.time.min() - (profile_df.time.min() % step_size)\n",
    ")\n",
    "sample_times = list(\n",
    "    range(int(min_profile_time), int(max_profile_time) + step_size, step_size)\n",
    ")\n",
    "prediction_time = data[\"time\"]\n",
    "\n",
    "test_df = data_df.copy()\n",
    "        \n",
    "# Look at the all the different coffeegrams that passed the threshold\n",
    "for val in weight_values:\n",
    "    median_output = df[df.coffeegrams == val].output.median()\n",
    "\n",
    "    # If there is no median value or output is greater than 5% diff, ignore\n",
    "    if np.isnan(median_output) or percent_diff(data[\"output\"], median_output) > 0.05:\n",
    "        continue\n",
    "    weight_df = data_df.copy()\n",
    "    weight_df[\"coffeegrams\"] = [val]\n",
    "    weight_df[\"output\"] = [median_output]\n",
    "    test_df = pd.concat([test_df, weight_df])\n",
    "\n",
    "# Recompute brewratio\n",
    "test_df[\"brewratio\"] = (\n",
    "    test_df.output / test_df.coffeegrams\n",
    ")\n",
    "\n",
    "for grinder in sorted(df.grinder.unique()):\n",
    "    if data[\"grinder\"] is not None and data[\"grinder\"].lower() != grinder.lower():\n",
    "        continue\n",
    "    test_grinder_df = test_df.copy()\n",
    "    test_grinder_df = test_grinder_df.assign(grinder=[grinder] * len(test_grinder_df))\n",
    "    grinder_df = df[\n",
    "        df.grinder == grinder\n",
    "    ]\n",
    "    if len(grinder_df) == 0:\n",
    "        print(\"No values for grinder\", grinder)\n",
    "        continue\n",
    "    fig = plt.figure(dpi=150)\n",
    "    fig.suptitle(f\"{grinder} Predictions\")\n",
    "    plot1 = fig.add_subplot(222)\n",
    "    plot2 = fig.add_subplot(224)\n",
    "    plot3 = fig.add_subplot(121)\n",
    "    plot1.set_xlabel(\"Prediction\")\n",
    "    plot1.set_yticks([])\n",
    "    plot2.set_ylabel(\"Coffee Grams\")\n",
    "    plot2.set_xlabel(\"Prediction\")\n",
    "\n",
    "    times = []\n",
    "    med = []\n",
    "    mean = []\n",
    "    if prediction_time not in sample_times:\n",
    "        sample_times.append(prediction_time)\n",
    "    \n",
    "    for time in sample_times:\n",
    "        test_grinder_df[\"time\"] = [time] * len(test_grinder_df)\n",
    "        \n",
    "        x_set, _ = prepare_df_for_modeling(test_grinder_df, \"grindersetting\", drop_cols=dropped, encoders=inference_encoders)\n",
    "        grams_idx = np.argwhere(x_set[-1] == test_grinder_df.coffeegrams.values[-1]).reshape(-1)\n",
    "        grinder_pred_fit = models_by_label[\"grindersetting\"].predict(x_set)\n",
    "        \n",
    "\n",
    "        if time == prediction_time:\n",
    "            grams_x = x_set[:, grams_idx].reshape(-1)\n",
    "            plot1.hist(grinder_pred_fit, bins=30, label=\"Predictions\", density=True)\n",
    "            plot2.scatter(grinder_pred_fit, grams_x, label=\"Predictions (grinder)\")\n",
    "            test_grinder_df[\"grindersetting\"] = grinder_pred_fit.tolist()\n",
    "            time_x_set, _ = prepare_df_for_modeling(test_grinder_df, \"time\", drop_cols=dropped, encoders=inference_encoders)\n",
    "            time_pred_fit = models_by_label[\"time\"].predict(time_x_set)\n",
    "            if len(grinder_pred_fit):\n",
    "                plot3.scatter(\n",
    "                    grinder_pred_fit,\n",
    "                    time_pred_fit,\n",
    "                    label=\"Predictions\",\n",
    "                    marker=\"o\",\n",
    "                    alpha=0.15,\n",
    "                )\n",
    "        else:\n",
    "            mean_pred = np.mean(grinder_pred_fit)\n",
    "            med_pred = np.median(grinder_pred_fit)\n",
    "            mean.append(mean_pred)\n",
    "            med.append(med_pred)\n",
    "            times.append(time)\n",
    "    def coffeegrams_filter(grams: float):\n",
    "        return percent_diff(data[\"coffeegrams\"], grams) <= 0.1\n",
    "\n",
    "    def output_filter(output: float):\n",
    "        return percent_diff(data[\"output\"], output) <= 0.1\n",
    "\n",
    "    mask = grinder_df.coffeegrams.apply(coffeegrams_filter)\n",
    "    output_mask = grinder_df.output.apply(output_filter)\n",
    "    ref_df = grinder_df[mask & output_mask]\n",
    "    \n",
    "    reference_vals = ref_df[\n",
    "        (ref_df.coffee == test_df.coffee.values[0])\n",
    "        & (ref_df.roaster == test_df.roaster.values[0])\n",
    "        & (ref_df.roastdate == test_df.roastdate.values[0])\n",
    "        & (ref_df.machine.str.lower() == test_df.machine.str.lower().values[0])\n",
    "        & (ref_df.grinder == grinder)\n",
    "        & (ref_df.machineprofile.str.lower() == test_df.machineprofile.str.lower().values[0])\n",
    "        & (ref_df.basket.str.lower() == test_df.basket.str.lower().values[0])\n",
    "        & (ref_df[\"filter\"].str.lower() == test_df[\"filter\"].str.lower().values[0])\n",
    "    ]\n",
    "    if len(reference_vals):\n",
    "        plot3.scatter(\n",
    "            reference_vals.grindersetting,\n",
    "            reference_vals.time,\n",
    "            label=\"Reference\",\n",
    "            marker=\"x\",\n",
    "            alpha=0.5,\n",
    "        )\n",
    "    \n",
    "    plot3.scatter(med, times, label=\"Median\", alpha=0.5)\n",
    "    plot3.scatter(mean, times, label=\"Mean\", alpha=0.5)\n",
    "\n",
    "    plot3.set_ylabel(\"Time\")\n",
    "    plot3.set_xlabel(\"Grinder setting\")\n",
    "    plot3.get_xaxis().set_minor_locator(AutoMinorLocator())\n",
    "    plot3.legend()\n",
    "    plot3.set_axisbelow(True)\n",
    "    plot3.grid(linestyle=\"--\", which=\"both\", alpha=.25)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
