{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "floating-madness",
   "metadata": {},
   "source": [
    "# Evaluating Simpler Models on collected data\n",
    "\n",
    "Because a Neural Net seems like overkill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "increasing-technique",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-21T16:05:29.344904Z",
     "iopub.status.busy": "2022-02-21T16:05:29.344565Z",
     "iopub.status.idle": "2022-02-21T16:05:29.841299Z",
     "shell.execute_reply": "2022-02-21T16:05:29.840936Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "from collections import defaultdict\n",
    "from tempfile import NamedTemporaryFile\n",
    "from typing import List, Tuple\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "from IPython.core.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sweet-finder",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-21T16:05:29.843748Z",
     "iopub.status.busy": "2022-02-21T16:05:29.843460Z",
     "iopub.status.idle": "2022-02-21T16:05:29.844668Z",
     "shell.execute_reply": "2022-02-21T16:05:29.844892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Disable GPU, fast enough without\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "DATA_DOWNLOAD_URL = \"https://docs.google.com/spreadsheets/d/1dDfuIq74pjELNtfb_0sdiNnG3qFKrfqC2sax7ewrWYg/export?format=csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e5a94a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-21T16:05:29.851090Z",
     "iopub.status.busy": "2022-02-21T16:05:29.850779Z",
     "iopub.status.idle": "2022-02-21T16:05:29.852341Z",
     "shell.execute_reply": "2022-02-21T16:05:29.852051Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_datetime_to_epoch_time(date):\n",
    "    return (date.astype(\"uint64\") / 1e9).astype(\"uint32\")\n",
    "\n",
    "def prepare_df_for_gp(frame, prediction_column=\"grindersetting\", encoders={}, drop_cols=None):\n",
    "    if drop_cols is None:\n",
    "        drop_cols = []\n",
    "    new_frame = frame.copy(deep=True)\n",
    "    for col in drop_cols:\n",
    "        try:\n",
    "            new_frame = new_frame.drop([col], axis=1)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for col, encoder in encoders.items():\n",
    "        if col not in new_frame:\n",
    "            if col not in drop_cols:\n",
    "                print(\"No such column:\", col)\n",
    "            continue\n",
    "        vals = encoder.transform(np.asarray(new_frame[col]).reshape(-1, 1))\n",
    "        new_frame[col] = [vals[i] for i in range(len(vals))] \n",
    "        \n",
    "    \n",
    "    y = np.asarray(new_frame[prediction_column])\n",
    "    for drop_col in [prediction_column]:\n",
    "        try:\n",
    "            new_frame = new_frame.drop(drop_col, axis=1)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    # Sort the columns so the ordering is consistent\n",
    "    cols = list(sorted(new_frame.columns.tolist()))\n",
    "    x = np.zeros((len(new_frame), len(new_frame.columns)), dtype=object)\n",
    "    frame_array = new_frame[cols].to_numpy()\n",
    "    x = np.zeros((frame_array.shape[0], np.hstack(frame_array[0]).shape[0]))\n",
    "    # Surely there is a way to vectorize this?\n",
    "    for i in range(frame_array.shape[0]):\n",
    "        x[i] = np.hstack(frame_array[i])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def find_columns_to_drop(df, skipped_columns=None):\n",
    "    if skipped_columns is None:\n",
    "        skipped_columns = set()\n",
    "    droppable_columns = []\n",
    "    for col in df.columns:\n",
    "        if col in skipped_columns:\n",
    "            continue\n",
    "        # Drop any column that is mostly NaNs\n",
    "        if np.count_nonzero(df[col].isna()) > 0.5 * len(df[col]):\n",
    "            droppable_columns.append(col)\n",
    "    return droppable_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bottom-tribune",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-21T16:05:29.859518Z",
     "iopub.status.busy": "2022-02-21T16:05:29.859206Z",
     "iopub.status.idle": "2022-02-21T16:05:30.802454Z",
     "shell.execute_reply": "2022-02-21T16:05:30.802684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping timeprediction\n",
      "Dropping tdsatagounfiltered\n",
      "Dropping tdstempc\n",
      "Encoding grinder\n",
      "Encoding coffee\n",
      "Encoding roaster\n",
      "Encoding machine\n",
      "Encoding machineprofile\n",
      "Dropping details\n",
      "Dropping arbitraryrating1-10\n"
     ]
    }
   ],
   "source": [
    "with NamedTemporaryFile(suffix=\".csv\") as temp:\n",
    "    with open(temp.name, \"wb\") as ofs:\n",
    "        resp = requests.get(DATA_DOWNLOAD_URL, stream=True)\n",
    "        for chunk in resp.iter_content():\n",
    "            ofs.write(chunk)\n",
    "    df = pd.read_csv(temp.name)\n",
    "for char in [' ', '(', ')', \"%\", \"/\", \"\\\\\", '\"']:\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(char, '')\n",
    "invalid_indices = df[df[\"time\"].isna()].index\n",
    "df.drop(invalid_indices, inplace=True)\n",
    "df = df[pd.to_numeric(df[\"grindersetting\"], errors=\"coerce\").notnull()]\n",
    "df[\"roastdate\"] = pd.to_datetime(df.roastdate, infer_datetime_format=True, utc=True).map(convert_datetime_to_epoch_time)\n",
    "df[\"date\"] = pd.to_datetime(df.date, infer_datetime_format=True, utc=True).map(convert_datetime_to_epoch_time)\n",
    "df[\"timesinceroast\"] = df[\"date\"] - df[\"roastdate\"]\n",
    "df[\"grindersetting\"] = df[\"grindersetting\"].astype(np.float64)\n",
    "df[[\"roaster\", \"coffee\"]] = df[\"coffee\"].str.lower().str.split('-', 1, expand=True)\n",
    "df[\"coffee\"] = df[\"coffee\"].astype(str).replace(\"None\", \"\").str.strip()\n",
    "df[\"machineprofile\"] = df[\"machineprofile\"].fillna(\"\").replace(\"\", \"default\").str.lower()\n",
    "df[\"roaster\"] = df[\"roaster\"].str.strip()\n",
    "df[\"brewratio\"] = df[\"output\"] / df[\"coffeegrams\"] \n",
    "\n",
    "encoded_cols = (\"grinder\", \"coffee\", \"roaster\", \"machine\", \"machineprofile\")\n",
    "\n",
    "for col in find_columns_to_drop(df):\n",
    "    if col in encoded_cols:\n",
    "        continue\n",
    "    print(\"Dropping\", col)\n",
    "    df = df.drop(col, axis=1)\n",
    "\n",
    "encs = {}\n",
    "for col in encoded_cols:\n",
    "    if col not in df:\n",
    "        print(\"No such column to encode\", col)\n",
    "        continue\n",
    "    if len(df[col].unique()) <= 1:\n",
    "        print(f\"Column {col} has 1 or fewer values, not encoding\")\n",
    "        continue\n",
    "#     encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    print(\"Encoding\", col)\n",
    "    encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=len(df[col].unique()))\n",
    "    encoder.fit(np.asarray(df[col]).reshape(-1, 1))\n",
    "    encs[col] = encoder\n",
    "\n",
    "# Drop fields that aren't going to be encoded or are a pandas type\n",
    "for field in df.columns:\n",
    "    if field in encs:\n",
    "        continue\n",
    "    if df[field].dtype == object:\n",
    "        print(\"Dropping\", field)\n",
    "        df = df.drop([field], axis=1)\n",
    "\n",
    "for col in [\"arbitraryrating1-10\"]:\n",
    "    if col in df.columns:\n",
    "        print(\"Dropping\", col)\n",
    "        df = df.drop(col, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f34aa1a",
   "metadata": {},
   "source": [
    "## Use distribution of grams of coffee in to enhance data\n",
    "\n",
    "Initially values were just the grams weighed before grinding. After getting a more precise scale, data has been collected after grinding/prep, want to see if using that data can enhance the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b06ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_field(frame, field: str, groupby, samples: int = 10, seed: int = 814):\n",
    "    to_sample = []\n",
    "    state = np.random.RandomState(seed)\n",
    "    # Duplicate a bunch of samples\n",
    "    for name, subdf in frame.groupby(groupby):\n",
    "        std = np.std(subdf[field])\n",
    "        if std == 0:\n",
    "            std = 0.01\n",
    "        to_return = pd.concat([subdf] * samples)\n",
    "        noise = state.normal(0.0, std * 0.5, size=len(to_return))\n",
    "        to_return[field] = (to_return[field].values + noise).tolist()\n",
    "        if field in [\"coffeegrams\", \"output\"]:\n",
    "            to_return.brewratio = to_return.output / to_return.coffeegrams\n",
    "        to_sample.append(to_return)\n",
    "    return pd.concat(to_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5760854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def construct_regressor(estimators=1700):\n",
    "    # Optimized using skopt\n",
    "    return  HistGradientBoostingRegressor(\n",
    "        loss=\"poisson\",\n",
    "        max_iter=estimators,\n",
    "        random_state=814\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-gender",
   "metadata": {},
   "source": [
    "## Evaluate using GradientBoosting to make predictions\n",
    "\n",
    "It is feasible to build a personalized GradientBoost model, and this is to do a basic proof of concept to see if it isn't totally worthless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-stanford",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-21T16:05:30.812787Z",
     "iopub.status.busy": "2022-02-21T16:05:30.809985Z",
     "iopub.status.idle": "2022-02-21T16:06:10.806146Z",
     "shell.execute_reply": "2022-02-21T16:06:10.806403Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction grindersetting\n",
      "With Dropped columns: ('output', 'date', 'roastdate')\n"
     ]
    }
   ],
   "source": [
    "# Samples values for the training set\n",
    "enhance_sample = True\n",
    "k_folds = False\n",
    "\n",
    "estimators = 1700\n",
    "for prediction_field in [\"grindersetting\"]:\n",
    "    print(\"Prediction\", prediction_field)\n",
    "    for dropped in [(\"output\", \"date\", \"roastdate\")]:\n",
    "        print(\"With Dropped columns:\", dropped)\n",
    "        mae_scores = []\n",
    "        mse_scores = []\n",
    "        fig, (ax, ax2) = plt.subplots(ncols=2, dpi=150, figsize=(9, 4.5))\n",
    "        data = df\n",
    "        true_vals = None\n",
    "        pred = None\n",
    "        if k_folds:\n",
    "            regr = construct_regressor()\n",
    "            folds = KFold(n_splits=5, random_state=814, shuffle=True)\n",
    "            for train_idx, test_idx in folds.split(data):\n",
    "                train = data.iloc[train_idx]\n",
    "                test =  data.iloc[test_idx]\n",
    "                if enhance_sample:\n",
    "                    sampled_coffee = resample_field(train, \"coffeegrams\", [\"machine\", \"grinder\", \"coffee\"])\n",
    "                    sampled_output = resample_field(train, \"output\", [\"machine\", \"machineprofile\"])\n",
    "                    train = pd.concat([train, sampled_coffee, sampled_output])\n",
    "                x_train, y_train = prepare_df_for_gp(train, prediction_column=prediction_field, drop_cols=dropped, encoders=encs)\n",
    "                regr.fit(x_train, y_train)\n",
    "                \n",
    "                x_test, y_test = prepare_df_for_gp(test, prediction_column=prediction_field, drop_cols=dropped, encoders=encs)\n",
    "\n",
    "                yfit = regr.predict(x_test)\n",
    "                ax.scatter(\n",
    "                    y_test,\n",
    "                    yfit,\n",
    "                    marker=\"o\",\n",
    "                    alpha=0.25,\n",
    "                )\n",
    "                if true_vals is None:\n",
    "                    true_vals = y_test\n",
    "                    pred = yfit\n",
    "                else:\n",
    "                    true_vals = np.concatenate([true_vals, y_test])\n",
    "                    pred = np.concatenate([pred, yfit])\n",
    "                mae_scores.append(mean_absolute_error(y_test, yfit))\n",
    "                mse_scores.append(mean_squared_error(y_test, yfit))\n",
    "        else:\n",
    "            latest_date = data.date.max()\n",
    "            num_days = 4\n",
    "            days_in_seconds = (24 * 60 * 60 * num_days)\n",
    "            # Training data is everything a week before the most recent pull\n",
    "            train = data[data.date < latest_date - days_in_seconds]\n",
    "            test = data.drop(train.index)\n",
    "            if enhance_sample:\n",
    "                sampled_coffee = resample_field(train, \"coffeegrams\", [\"machine\", \"grinder\", \"coffee\"])\n",
    "                sampled_output = resample_field(train, \"output\", [\"machine\", \"machineprofile\"])\n",
    "                train = pd.concat([train, sampled_coffee, sampled_output])\n",
    "            x, y = prepare_df_for_gp(train, prediction_column=prediction_field, drop_cols=dropped, encoders=encs)\n",
    "            regr = construct_regressor(estimators=estimators)\n",
    "            regr.fit(x, y)\n",
    "            x_test, y_test = prepare_df_for_gp(test, prediction_column=prediction_field, drop_cols=dropped, encoders=encs)\n",
    "            yfit = regr.predict(x_test)\n",
    "            ax.scatter(\n",
    "                y_test,\n",
    "                yfit,\n",
    "                marker=\"o\",\n",
    "                alpha=0.25,\n",
    "            )\n",
    "            if true_vals is None:\n",
    "                true_vals = y_test\n",
    "                pred = yfit\n",
    "            else:\n",
    "                true_vals = np.concatenate([true_vals, y_test])\n",
    "                pred = np.concatenate([pred, yfit])\n",
    "            mae_scores.append(mean_absolute_error(y_test, yfit))\n",
    "            mse_scores.append(mean_squared_error(y_test, yfit))\n",
    "        ax.set_title(f\"Prediction of Gradient Boosting Model\\n{prediction_field}\")\n",
    "        ax.set_xlabel(\"Experimental\")\n",
    "        ax.set_ylabel(\"Prediction\")\n",
    "        ax.axline([0, 0], [1, 1], color=\"g\", label=\"fit\")\n",
    "        ax.annotate(\n",
    "            \"mse = {:.2f}\".format(mean_squared_error(true_vals, pred)),\n",
    "            (1.03, 0.01),\n",
    "            xycoords=\"axes fraction\",\n",
    "        )\n",
    "        ax.annotate(\n",
    "            \"mae = {:.2f}\".format(mean_absolute_error(true_vals, pred)),\n",
    "            (1.03, 0.07),\n",
    "            xycoords=\"axes fraction\",\n",
    "        )\n",
    "\n",
    "        ax2.hist(pred - true_vals, bins=30, density=True)\n",
    "        ax2.set_title(f\"Prediction Errors\\n{prediction_field}\")\n",
    "        ax2.set_xlabel(\"Error\")\n",
    "        ax2.set_ylabel(\"Density\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-excellence",
   "metadata": {},
   "source": [
    "## Trying it out on new data\n",
    "\n",
    "What happens if we throw 'novel' coffee at it, how well will it predict it across grinders?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-rogers",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-21T16:06:10.818832Z",
     "iopub.status.busy": "2022-02-21T16:06:10.815434Z",
     "iopub.status.idle": "2022-02-21T16:06:17.702901Z",
     "shell.execute_reply": "2022-02-21T16:06:17.702589Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dropped = (\"roastdate\", \"date\", \"brewratio\")\n",
    "enhance_sample = True\n",
    "\n",
    "data = {\n",
    "    \"coffee\": \"Cog - Guatemala Huehuetenago 62\",\n",
    "    \"coffeegrams\": 18.0,\n",
    "    \"roastdate\": \"08/8/22\",\n",
    "    \"time\": 37,\n",
    "    \"brewratio\": 2.0,\n",
    "    \"machine\": \"Decent DE1PRO\",\n",
    "    \"machineprofile\": \"default\",\n",
    "    \"grinder\": None, # Leave None to predict for all grinders\n",
    "}\n",
    "training = df\n",
    "if enhance_sample:\n",
    "    sampled_coffee = resample_field(training, \"coffeegrams\", [\"machine\", \"grinder\", \"coffee\"])\n",
    "    sampled_output = resample_field(training, \"output\", [\"machine\", \"machineprofile\", \"grinder\", \"coffee\"])\n",
    "    training = pd.concat([training, sampled_coffee, sampled_output])\n",
    "\n",
    "\n",
    "\n",
    "data[\"date\"] = date.today().strftime(\"%m/%d/%Y\")\n",
    "data[\"grindersetting\"] = 0.5 # Meaningless value\n",
    "data[\"output\"] = np.round(data[\"brewratio\"] * data[\"coffeegrams\"], 1)\n",
    "test_df = pd.DataFrame.from_dict([data])\n",
    "test_df[[\"roaster\", \"coffee\"]] = test_df[\"coffee\"].str.lower().str.split('-', 1, expand=True)\n",
    "test_df[\"coffee\"] = test_df[\"coffee\"].str.replace(\"None\", \"\").str.strip()\n",
    "test_df[\"roaster\"] = test_df[\"roaster\"].str.strip()\n",
    "if test_df[\"roaster\"].values[0] not in df.roaster.values:\n",
    "    print(\"No matching roaster found\", test_df[\"roaster\"].values[0])\n",
    "    print(\"-----\")\n",
    "if test_df[\"coffee\"].values[0] not in df.coffee.values:\n",
    "    print(\"No matching coffee found\", test_df[\"coffee\"].values[0])\n",
    "    print(\"-----\")\n",
    "test_df.roastdate = pd.to_datetime(test_df.roastdate, infer_datetime_format=True, utc=True).map(convert_datetime_to_epoch_time)\n",
    "test_df.date = pd.to_datetime(test_df.date, infer_datetime_format=True, utc=True).map(convert_datetime_to_epoch_time)\n",
    "test_df[\"timesinceroast\"] = test_df.date - test_df.roastdate\n",
    "\n",
    "x, y = prepare_df_for_gp(training, drop_cols=dropped, encoders=encs)\n",
    "regr = construct_regressor(1700)\n",
    "regr.fit(x, y)\n",
    "for grinder in df.grinder.unique():\n",
    "    if data[\"grinder\"] is not None and data[\"grinder\"].lower() != grinder.lower():\n",
    "        continue\n",
    "    test_grinder_df = test_df.copy()\n",
    "    test_grinder_df = test_grinder_df.assign(grinder=[grinder])\n",
    "    grinder_df = df[df.grinder == grinder] # & (df.machine == test_grinder_df.machine.values[0])]\n",
    "    if len(grinder_df) == 0:\n",
    "        continue\n",
    "    fig = plt.figure(dpi=150)\n",
    "    fig.suptitle(f\"{grinder} Predictions\")\n",
    "    plot1 = fig.add_subplot(222)\n",
    "    plot2 = fig.add_subplot(224)\n",
    "    plot3 = fig.add_subplot(121)\n",
    "    plot1.set_xlabel(\"Prediction\")\n",
    "    plot1.set_yticks([])\n",
    "    plot2.set_xlabel(\"Coffee Grams\")\n",
    "    plot2.set_ylabel(\"Grinder Prediction\")\n",
    "\n",
    "    times = []\n",
    "    med = []\n",
    "    mean = []\n",
    "    profile_df = grinder_df[\n",
    "        (grinder_df.machine == test_df.machine.values[0]) &\n",
    "        (grinder_df.machineprofile == test_df.machineprofile.values[0])\n",
    "    ]\n",
    "    weight_values = grinder_df.coffeegrams.unique()\n",
    "    print(len(weight_values))\n",
    "    \n",
    "    max_profile_time = min(60, profile_df.time.max())\n",
    "    step_size = 5\n",
    "    min_profile_time = max(step_size, profile_df.time.min() - profile_df.time.min() % step_size)\n",
    "    sample_times = list(range(int(min_profile_time), int(max_profile_time) + step_size, step_size))\n",
    "    prediction_time = data[\"time\"]\n",
    "    if prediction_time not in sample_times:\n",
    "        sample_times.append(prediction_time)\n",
    "    for time in sample_times:\n",
    "        test_grinder_df[\"time\"] = [time]\n",
    "        x_set = [prepare_df_for_gp(test_grinder_df, drop_cols=dropped, encoders=encs)[0][0]]\n",
    "        # Look at the all the different coffeegrams values within a threshold\n",
    "        for val in weight_values:\n",
    "            if np.abs(data[\"coffeegrams\"] - val) >= 0.1:\n",
    "                continue\n",
    "            median_output = profile_df[profile_df.coffeegrams == val].output.median()\n",
    "            if np.isnan(median_output) or np.abs(median_output - data[\"output\"]) > 2.5:\n",
    "                continue\n",
    "            test_grinder_df[\"coffeegrams\"] = [val]\n",
    "            test_grinder_df[\"output\"] = [median_output]\n",
    "            test_grinder_df[\"brewratio\"] = test_grinder_df.output / test_grinder_df.coffeegrams\n",
    "            x_test, _ = prepare_df_for_gp(test_grinder_df, drop_cols=dropped, encoders=encs)\n",
    "            x_set.append(x_test[0])\n",
    "            grams_idx = np.argwhere(x_set[-1] == val).reshape(-1)\n",
    "        x_set = np.array(x_set)\n",
    "        yfit = regr.predict(x_set)\n",
    "        \n",
    "        if time == prediction_time:\n",
    "            grams_x = x_set[:, grams_idx].reshape(-1)\n",
    "            plot1.hist(yfit, bins=min(100, len(yfit) // 3), label=\"Predictions\", density=True)\n",
    "            plot2.scatter(grams_x, yfit, label=\"Predictions\")\n",
    "            if len(yfit):\n",
    "                plot3.scatter(yfit, [time] * len(yfit), label=\"Predictions\", marker=\"o\", alpha=0.25)\n",
    "        else:\n",
    "            mean_pred = np.mean(yfit)\n",
    "            med_pred = np.median(yfit)\n",
    "            mean.append(mean_pred)\n",
    "            med.append(med_pred)\n",
    "            times.append(time)\n",
    "            \n",
    "\n",
    "    plot3.scatter(med, times, label=\"Median\", alpha=0.5)\n",
    "    plot3.scatter(mean, times, label=\"Mean\", alpha=0.5)\n",
    "    \n",
    "    reference_vals = grinder_df[\n",
    "        (grinder_df.coffee == test_df.coffee.values[0]) &\n",
    "        (grinder_df.roaster == test_df.roaster.values[0]) &\n",
    "        (grinder_df.roastdate == test_df.roastdate.values[0]) &\n",
    "        (grinder_df.machine == test_df.machine.values[0]) &\n",
    "        (grinder_df.grinder == grinder) &\n",
    "        (grinder_df.machineprofile == test_df.machineprofile.values[0])\n",
    "    ]\n",
    "    if len(reference_vals):\n",
    "        plot3.scatter(reference_vals.grindersetting, reference_vals.time, label=\"Reference\", marker=\"x\")\n",
    "\n",
    "    plot3.set_ylabel(\"Time\")\n",
    "    plot3.set_xlabel(\"Grinder setting\")\n",
    "    plot3.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314cdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
